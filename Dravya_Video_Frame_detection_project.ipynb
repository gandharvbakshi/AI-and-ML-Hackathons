{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dravya Video Frame detection project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1GAYQoorVvrD_9g5G7X1kUMA3XCIvcU8N",
      "authorship_tag": "ABX9TyO9KgMR2tG0W/J1R1/2pVzR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gandharvbakshi/AI-and-ML-Hackathons/blob/main/Dravya_Video_Frame_detection_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38oK7zTDz-Wo",
        "outputId": "f5baf37c-7cdc-40b0-e1ce-73a313a17c79"
      },
      "source": [
        "import matplotlib.pylab as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "print(\"Tensorflow version\", tf.__version__)\n",
        "from tensorflow import keras\n",
        "print(\"Keras version\", keras.__version__)\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42) \n",
        "tf.random.set_seed(42)\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "import cv2 \n",
        "print(\"CV2 version\", cv2.__version__)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version 2.6.0\n",
            "Keras version 2.6.0\n",
            "CV2 version 4.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSB2X2rc6eQk"
      },
      "source": [
        "dataDir = \"/content/drive/MyDrive/AI ML Projects/Dravya/dataset\"\n",
        "trainDataRaw = pd.read_csv(\"/content/drive/MyDrive/AI ML Projects/Dravya/dataset/train.csv\")\n",
        "testDataRaw = pd.read_csv(\"/content/drive/MyDrive/AI ML Projects/Dravya/dataset/test.csv\")\n"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "lJDgZSKWcHwe",
        "outputId": "29e7b98c-3fda-478c-fd4f-fa0232480247"
      },
      "source": [
        "testDataRaw.tail(2)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Frame_ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>Test_111.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>Test_112.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Frame_ID\n",
              "111  Test_111.jpg\n",
              "112  Test_112.jpg"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0d8SA0QrVh8"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "  \n",
        "# Function to extract frames\n",
        "def FrameCapture(dataDir, path, stop_count, name):\n",
        "      \n",
        "    # Path to video file\n",
        "    vidObj = cv2.VideoCapture(path)\n",
        "    fps = vidObj.get(cv2.CAP_PROP_FPS)\n",
        "    print(\"Frames per second: \",fps)\n",
        "    # Used as counter variable\n",
        "    count = 0\n",
        "  \n",
        "    # checks whether frames were extracted\n",
        "    success = 1\n",
        "  \n",
        "    while success:\n",
        "  \n",
        "        # vidObj object calls read\n",
        "        # function extract frames\n",
        "        vidObj.set(cv2.CAP_PROP_POS_MSEC,(count*1000)) \n",
        "        success, image = vidObj.read()\n",
        "        if (success == False):\n",
        "          break\n",
        "        if (count > stop_count):\n",
        "          break\n",
        "        if os.path.isfile(os.path.join(dataDir, name + \"_%d.jpg\" % count)):\n",
        "          os.remove(os.path.join(dataDir, name + \"_%d.jpg\" % count)) \n",
        "        # Saves the frames with frame-count\n",
        "        cv2.imwrite(os.path.join(dataDir, name + \"_%d.jpg\" % count), image)\n",
        "  \n",
        "        count += 1"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2Xy0pJ4_R1U"
      },
      "source": [
        "def FrameCounter(path):\n",
        "      \n",
        "    # Path to video file\n",
        "    vidObj = cv2.VideoCapture(path)\n",
        "    fps = vidObj.get(cv2.CAP_PROP_FPS)\n",
        "    print(\"Frames per second: \",fps)\n",
        "    # Used as counter variable\n",
        "    count = 0\n",
        "  \n",
        "    # checks whether frames were extracted\n",
        "    success = 1\n",
        "  \n",
        "    while success:\n",
        "  \n",
        "        # vidObj object calls read\n",
        "        # function extract frames\n",
        "        vidObj.set(cv2.CAP_PROP_POS_MSEC,(count*1000)) \n",
        "        success, image = vidObj.read()\n",
        "        if (success == False):\n",
        "          break\n",
        "        #if (count > stop_count):\n",
        "        #  break\n",
        "\n",
        "        count += 1\n",
        "    return count"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twVlbE8drqZa",
        "outputId": "1cfb50ed-ff33-4166-dd94-c40c98a03e16"
      },
      "source": [
        "trainVideoPath = os.path.join(dataDir, 'train.mp4')\n",
        "testVideoPath = os.path.join(dataDir, 'test.mp4')\n",
        "#os.mkdir(os.path.join(dataDir, \"train\"))\n",
        "trainImageDir = os.path.join(dataDir, \"train\")\n",
        "#os.mkdir(os.path.join(dataDir, \"test\"))\n",
        "testImageDir = os.path.join(dataDir, \"test\")\n",
        "FrameCapture(trainImageDir, trainVideoPath, 407, \"Frame\")\n",
        "FrameCapture(testImageDir, testVideoPath, 112, \"Test\")"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frames per second:  29.97002997002997\n",
            "Frames per second:  24.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ohfPXcR_hSU",
        "outputId": "7cacedae-9fc1-43e8-b56b-f57f9f3eae4e"
      },
      "source": [
        "FrameCounter(testVideoPath)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frames per second:  24.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "113"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "-xCjdDd7xP2L",
        "outputId": "69d3461a-7e34-47f1-fefa-048b66049661"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le_class = LabelEncoder()\n",
        "trainDataRaw['Animal'] = le_class.fit_transform(trainDataRaw['Animal'])\n",
        "trainDataRaw.head(2)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Frame_ID</th>\n",
              "      <th>Animal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Frame_0.jpg</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Frame_1.jpg</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Frame_ID  Animal\n",
              "0  Frame_0.jpg       5\n",
              "1  Frame_1.jpg       5"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EyMME_Za1zk",
        "outputId": "65bdab6a-6ba4-40fe-c56a-45a07b14bd4c"
      },
      "source": [
        "#Current Image size - 320 * 240 ; new size - 224 * 224 \n",
        "IMG_SIZE = 224 \n",
        "X = [] \n",
        "y = [] \n",
        "i = 0\n",
        "\n",
        "for img in trainDataRaw[\"Frame_ID\"].tolist():\n",
        "  if (i % 1000 == 0):\n",
        "    print(i, \"th image done\")\n",
        "  img_array = cv2.imread(os.path.join(trainImageDir,img ))\n",
        "  #fsa.append(img_array)\n",
        "  i = i + 1\n",
        "  new_array = cv2.resize(img_array, (IMG_SIZE,IMG_SIZE))\n",
        "  X.append(new_array)\n",
        "  #img_array = cv2.normalize(img_array, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "  #X_scaled = np.append(X_scaled, img_array) #.append()\n",
        "  y.append(trainDataRaw.loc[trainDataRaw[\"Frame_ID\"]==img, 'Animal'])\n",
        "  #plt.imshow(img_array)\n",
        "  #plt.show()\n",
        "  #break\n",
        "X = np.array(X)\n",
        "#X_scaled = np.array(X_scaled)\n",
        "y = np.array(y)  "
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 th image done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltu8KNXncakQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8051f49f-a834-4509-cac0-a0a68437e5f6"
      },
      "source": [
        "i = 0\n",
        "X_test = []\n",
        "\n",
        "for img in testDataRaw[\"Frame_ID\"].tolist():\n",
        "  if (i % 1000 == 0):\n",
        "    print(i, \"th image done\")\n",
        "  img_array = cv2.imread(os.path.join(testImageDir,img ))\n",
        "  if (os.path.isfile(os.path.join(testImageDir,img)) == False):\n",
        "    print(\"File not found: \", testImageDir, img)\n",
        "\n",
        "  i = i + 1\n",
        "  new_array = cv2.resize(img_array, (IMG_SIZE,IMG_SIZE))\n",
        "  X_test.append(new_array)\n",
        "X_test = np.array(X_test)\n"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 th image done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lhFjNp50jAS",
        "outputId": "78e9fea2-e9ee-4666-d97d-c3e12d55445a"
      },
      "source": [
        "y = pd.concat((trainDataRaw, pd.get_dummies(trainDataRaw['Animal'], prefix = 'Animal_')), axis=1).drop(['Frame_ID', 'Animal'],axis = 1).to_numpy()\n",
        "y.shape"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(408, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFlte5DzdVi9",
        "outputId": "43db9454-5028-4539-85bd-62a2b846f909"
      },
      "source": [
        "print(X.shape, y.shape, X_test.shape)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(408, 224, 224, 3) (408, 13) (113, 224, 224, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LahEqJDdYhl"
      },
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.applications.vgg16 import decode_predictions\n",
        "\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.applications.inception_v3 import decode_predictions\n",
        "\n",
        "#from keras.applications.resnet50 import ResNet50\n",
        "#from keras.applications.resnet50 import preprocess_input\n",
        "#from keras.applications.resnet50 import decode_predictions\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#import efficientnet.keras as efn\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def VGG16_model():\n",
        "  base_model = VGG16(include_top=False, input_shape=(224, 224, 3))\n",
        "  base_model.trainable = False\n",
        "  model = keras.models.Sequential([base_model,\n",
        "                                  keras.layers.GlobalAveragePooling2D(),\n",
        "                                  keras.layers.Dense(1024, activation='relu'),\n",
        "                                  #keras.layers.Dropout(0.2),\n",
        "                                  keras.layers.Dense(units=13, activation='softmax')                                     \n",
        "                                  ])\n",
        "  base_learning_rate = 0.00001\n",
        "  model.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])  \n",
        "  return model\n",
        "\n",
        "def Inception_model():\n",
        "  base_model = InceptionV3(include_top=False, input_shape=(224, 224, 3))\n",
        "  base_model.trainable = False\n",
        "  model = keras.models.Sequential([base_model,\n",
        "                                  keras.layers.GlobalAveragePooling2D(),\n",
        "                                  keras.layers.Dense(1024, activation='relu'),\n",
        "                                  #keras.layers.Dropout(0.2),\n",
        "                                  keras.layers.Dense(units=13, activation='softmax')\n",
        "                                  ])\n",
        "  base_learning_rate = 0.00001\n",
        "  model.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])  \n",
        "  return model\n",
        "\n",
        "def Inception_trainable_model():\n",
        "  base_model = InceptionV3(include_top=False, input_shape=(224, 224, 3))\n",
        "  #base_model.trainable = True\n",
        "  model = keras.models.Sequential([base_model,\n",
        "                                  keras.layers.GlobalAveragePooling2D(),\n",
        "                                  keras.layers.Dense(1024, activation='relu'),\n",
        "                                  #keras.layers.Dropout(0.2),\n",
        "                                  keras.layers.Dense(units=13, activation='softmax') \n",
        "                                  ])\n",
        "  base_learning_rate = 0.00001\n",
        "  model.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])  \n",
        "  return model\n",
        "\n",
        "\n",
        "def VGG16_trainable_true_model():\n",
        "  base_model = VGG16(include_top=False, input_shape=(224, 224, 3))\n",
        "  #base_model.trainable = False\n",
        "  model = keras.models.Sequential([base_model,\n",
        "                                  keras.layers.GlobalAveragePooling2D(),\n",
        "                                  keras.layers.Dense(1024, activation='relu'),\n",
        "                                  #keras.layers.Dropout(0.2),\n",
        "                                  keras.layers.Dense(units=13, activation='softmax')                                     \n",
        "                                  ])\n",
        "  base_learning_rate = 0.00001\n",
        "  model.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])  \n",
        "  return model\n",
        "\n",
        "def VGG16_slow_learn_model():\n",
        "  base_model = VGG16(include_top=False, input_shape=(224, 224, 3))\n",
        "  base_model.trainable = False\n",
        "  model = keras.models.Sequential([base_model,\n",
        "                                  keras.layers.GlobalAveragePooling2D(),\n",
        "                                  keras.layers.Dense(1024, activation='relu'),\n",
        "                                  #keras.layers.Dropout(0.2),\n",
        "                                  keras.layers.Dense(units=1, activation='sigmoid')                                     \n",
        "                                  ])\n",
        "  base_learning_rate = 0.000001\n",
        "  model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])  \n",
        "  return model\n",
        "\n",
        "def VGG16_fast_learn_model():\n",
        "  base_model = VGG16(include_top=False, input_shape=(224, 224, 3))\n",
        "  base_model.trainable = False\n",
        "  model = keras.models.Sequential([base_model,\n",
        "                                  keras.layers.GlobalAveragePooling2D(),\n",
        "                                  keras.layers.Dense(1024, activation='relu'),\n",
        "                                  #keras.layers.Dropout(0.2),\n",
        "                                  keras.layers.Dense(units=1, activation='sigmoid')                                     \n",
        "                                  ])\n",
        "  base_learning_rate = 0.0001\n",
        "  model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])  \n",
        "  return model\n",
        "\n",
        "def VGG16_dropout_model():\n",
        "  base_model = VGG16(include_top=False, input_shape=(224, 224, 3))\n",
        "  base_model.trainable = False\n",
        "  model = keras.models.Sequential([base_model,\n",
        "                                  keras.layers.GlobalAveragePooling2D(),\n",
        "                                  keras.layers.Dense(1024, activation='relu'),\n",
        "                                  keras.layers.Dropout(0.2),\n",
        "                                  keras.layers.Dense(units=1, activation='sigmoid')                                     \n",
        "                                  ])\n",
        "  base_learning_rate = 0.00001\n",
        "  model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])  \n",
        "  return model\n",
        "\n",
        "def VGG16_deeper_model():\n",
        "  base_model = VGG16(include_top=False, input_shape=(224, 224, 3))\n",
        "  base_model.trainable = False\n",
        "  model = keras.models.Sequential([base_model,\n",
        "                                  keras.layers.GlobalAveragePooling2D(),\n",
        "                                  keras.layers.Dense(1024, activation='relu'),\n",
        "                                  #keras.layers.Dropout(0.2),\n",
        "                                  keras.layers.Dense(1024, activation='relu'),\n",
        "                                  keras.layers.Dense(1024, activation='relu'),\n",
        "                                  keras.layers.Dense(units=1, activation='sigmoid')                                     \n",
        "                                  ])\n",
        "  base_learning_rate = 0.00001\n",
        "  model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])  \n",
        "  return model\n",
        "\n",
        "def VGG16_deeper_dropout_model():\n",
        "  base_model = VGG16(include_top=False, input_shape=(224, 224, 3))\n",
        "  base_model.trainable = False\n",
        "  model = keras.models.Sequential([base_model,\n",
        "                                  keras.layers.GlobalAveragePooling2D(),\n",
        "                                  keras.layers.Dense(1024, activation='relu'),\n",
        "                                  keras.layers.Dropout(0.2),\n",
        "                                  keras.layers.Dense(1024, activation='relu'),\n",
        "                                  keras.layers.Dropout(0.2),\n",
        "                                  keras.layers.Dense(1024, activation='relu'),\n",
        "                                  keras.layers.Dropout(0.2),\n",
        "                                  keras.layers.Dense(units=1, activation='sigmoid')                                     \n",
        "                                  ])\n",
        "  base_learning_rate = 0.00001\n",
        "  model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])  \n",
        "  return model\n",
        "\n",
        "def ResNet50_model():\n",
        "  base_model = ResNet50(include_top=False, input_shape=(224, 224, 3))\n",
        "  base_model.trainable = False\n",
        "  model = keras.models.Sequential([base_model,\n",
        "                                  keras.layers.GlobalAveragePooling2D(),\n",
        "                                  #keras.layers.Flatten(),\n",
        "                                  keras.layers.Dense(1024, activation='relu'),\n",
        "                                  keras.layers.Dropout(0.2),\n",
        "                                  keras.layers.Dense(1024, activation='relu'),\n",
        "                                  keras.layers.Dropout(0.2),\n",
        "                                  keras.layers.Dense(1024, activation='relu'),\n",
        "                                  #keras.layers.Dropout(0.2),\n",
        "                                  keras.layers.Dense(units=1, activation='sigmoid')                                     \n",
        "                                  ])\n",
        "  base_learning_rate = 0.00001\n",
        "  model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])  \n",
        "  return model\n",
        "\n",
        "def variable_model(FILTERS = 64,KERNEL_SIZE = 7):\n",
        "  # create model\n",
        "  DefaultConv2D = partial(keras.layers.Conv2D, kernel_size=KERNEL_SIZE, activation='relu', padding=\"SAME\")\n",
        "  model = keras.models.Sequential([\n",
        "    DefaultConv2D(filters=FILTERS, kernel_size=KERNEL_SIZE, input_shape=[224, 224, 3]), \n",
        "    keras.layers.MaxPooling2D(pool_size=2), \n",
        "    DefaultConv2D(filters=128),  \n",
        "    DefaultConv2D(filters=128),\n",
        "    keras.layers.MaxPooling2D(pool_size=2),\n",
        "    DefaultConv2D(filters=256),\n",
        "    DefaultConv2D(filters=256),\n",
        "    keras.layers.MaxPooling2D(pool_size=2),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(units=128, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(units=64, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(units=1, activation='sigmoid')\n",
        "])               \n",
        "  model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])    \n",
        "  return model\n",
        "\n",
        "def old_model():\n",
        "  # create model\n",
        "  DefaultConv2D = partial(keras.layers.Conv2D, kernel_size=3, activation='relu', padding=\"SAME\")\n",
        "  model = keras.models.Sequential([\n",
        "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[224, 224, 3]), \n",
        "    keras.layers.MaxPooling2D(pool_size=2), \n",
        "    DefaultConv2D(filters=128),  \n",
        "    DefaultConv2D(filters=128),\n",
        "    keras.layers.MaxPooling2D(pool_size=2),\n",
        "    DefaultConv2D(filters=256),\n",
        "    DefaultConv2D(filters=256),\n",
        "    keras.layers.MaxPooling2D(pool_size=2),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(units=128, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(units=64, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(units=13, activation='softmax')\n",
        "])               \n",
        "  model.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])    \n",
        "  return model\n",
        "\n"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InLEZbi4dmq1",
        "outputId": "c01cf4c7-6f9b-4370-f956-11a1ff27c9e8"
      },
      "source": [
        "#X = X.astype('float64')\n",
        "#X_test = X_test.astype('float64')\n",
        "#X /= 255\n",
        "#X_test /= 255\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "x_final = X_test\n",
        "print(len(x_train), \"train +\", len(x_test), \"test\")\n"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "326 train + 82 test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_i16ginN2gl3",
        "outputId": "773bf772-85ea-4737-bb6c-3e1714ec0d70"
      },
      "source": [
        "y_test\n",
        "ind = np.argmax(y_test, axis= 1)#, y_test.shape)\n",
        "ind"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 6,  0, 12,  9,  4,  6, 10,  1,  1, 12, 12,  6,  2, 12,  7,  7,  7,\n",
              "        0, 12,  7,  2, 12,  2,  0,  2,  8,  1,  0,  7, 12, 10,  3,  5, 10,\n",
              "       12, 12,  2,  8,  1,  9,  8, 12,  7, 12,  3,  8,  9, 12,  7, 10,  7,\n",
              "       12, 12,  4, 12,  4,  6,  6,  4, 12,  2,  7,  9,  7,  1,  7, 12,  3,\n",
              "        9,  2,  0,  1,  6,  7,  7, 12,  4,  1, 12,  1,  4,  1])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJ4DTMCZd0HC",
        "outputId": "5088f288-0a09-4169-c601-0821b869cc2e"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# defining the path to save the model\n",
        "best_model_path = os.path.join(dataDir, 'best_model.hdf5')\n",
        "\n",
        "model = old_model() #variable_model(32, 3) #old_model()\n",
        "my_callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5),\n",
        "    ModelCheckpoint(best_model_path, monitor='val_accuracy', verbose = 1, save_best_only = True, mode='max')\n",
        "]\n",
        "\n",
        "trial = model.fit(x_train,y_train,  epochs=131, validation_data = (x_test, y_test), callbacks=my_callbacks) #batch_size=batch_size,                      \n",
        "\n",
        "model.load_weights(best_model_path)\n",
        "y_bm_keras = model.predict(x_test)\n",
        "y_test_class = np.argmax(y_test, axis= 1)\n",
        "y_bm_keras_class = np.argmax(y_bm_keras, axis= 1)\n",
        "print(\"Train Accuracy Score \" , f1_score(y_test_class, y_bm_keras_class, average='micro') )\n"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/131\n",
            "11/11 [==============================] - 3s 134ms/step - loss: 60.8164 - accuracy: 0.1043 - val_loss: 2.4656 - val_accuracy: 0.0976\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.09756, saving model to /content/drive/MyDrive/AI ML Projects/Dravya/dataset/best_model.hdf5\n",
            "Epoch 2/131\n",
            "11/11 [==============================] - 1s 109ms/step - loss: 2.5410 - accuracy: 0.2086 - val_loss: 2.4816 - val_accuracy: 0.2317\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.09756 to 0.23171, saving model to /content/drive/MyDrive/AI ML Projects/Dravya/dataset/best_model.hdf5\n",
            "Epoch 3/131\n",
            "11/11 [==============================] - 1s 109ms/step - loss: 2.9118 - accuracy: 0.2209 - val_loss: 3.0807 - val_accuracy: 0.2317\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.23171\n",
            "Epoch 4/131\n",
            "11/11 [==============================] - 1s 108ms/step - loss: 2.6693 - accuracy: 0.2055 - val_loss: 2.3419 - val_accuracy: 0.2317\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.23171\n",
            "Epoch 5/131\n",
            "11/11 [==============================] - 1s 109ms/step - loss: 2.4002 - accuracy: 0.2607 - val_loss: 2.2753 - val_accuracy: 0.2317\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.23171\n",
            "Epoch 6/131\n",
            "11/11 [==============================] - 1s 109ms/step - loss: 2.3426 - accuracy: 0.2331 - val_loss: 2.2273 - val_accuracy: 0.2317\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.23171\n",
            "Epoch 7/131\n",
            "11/11 [==============================] - 1s 108ms/step - loss: 2.3411 - accuracy: 0.2577 - val_loss: 2.3107 - val_accuracy: 0.2317\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.23171\n",
            "Epoch 8/131\n",
            "11/11 [==============================] - 1s 109ms/step - loss: 2.4996 - accuracy: 0.2791 - val_loss: 1.9795 - val_accuracy: 0.3902\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.23171 to 0.39024, saving model to /content/drive/MyDrive/AI ML Projects/Dravya/dataset/best_model.hdf5\n",
            "Epoch 9/131\n",
            "11/11 [==============================] - 1s 109ms/step - loss: 2.0113 - accuracy: 0.3405 - val_loss: 1.6765 - val_accuracy: 0.3902\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.39024\n",
            "Epoch 10/131\n",
            "11/11 [==============================] - 1s 109ms/step - loss: 15.9382 - accuracy: 0.1963 - val_loss: 2.4627 - val_accuracy: 0.3049\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.39024\n",
            "Epoch 11/131\n",
            "11/11 [==============================] - 1s 108ms/step - loss: 2.8830 - accuracy: 0.1626 - val_loss: 2.3128 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.39024\n",
            "Epoch 12/131\n",
            "11/11 [==============================] - 1s 109ms/step - loss: 2.4340 - accuracy: 0.2362 - val_loss: 2.4585 - val_accuracy: 0.3415\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.39024\n",
            "Epoch 13/131\n",
            "11/11 [==============================] - 1s 108ms/step - loss: 2.7745 - accuracy: 0.2853 - val_loss: 4.1093 - val_accuracy: 0.0732\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.39024\n",
            "Epoch 14/131\n",
            "11/11 [==============================] - 1s 108ms/step - loss: 3.0733 - accuracy: 0.2331 - val_loss: 2.2395 - val_accuracy: 0.3293\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.39024\n",
            "Train Accuracy Score  0.3902439024390244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geJ9bgyy6ZLO",
        "outputId": "f8bec671-c740-4169-9745-ea8591aeb364"
      },
      "source": [
        "y_bm_keras_class"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7,  4, 12,  1, 10,  1,  7,  1,  1, 12,  6, 12,  1, 12, 12,  7, 12,\n",
              "       12,  2, 12,  7,  6,  1,  1,  4,  9, 12,  0,  0,  6,  1,  1,  2, 12,\n",
              "        7, 12,  7, 12,  2,  2,  6,  7,  7, 10, 12,  0,  4,  4, 12,  5, 12,\n",
              "       12,  7, 12,  8,  1, 12,  1,  1, 12,  7, 10, 10,  7,  0,  7,  1,  2,\n",
              "        3,  7,  1, 10,  7,  6,  4,  9, 10, 12,  1, 12,  0,  2])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuyuEdME-q2d"
      },
      "source": [
        "y_bm_final = model.predict(x_final)\n",
        "y_bm_final = np.argmax(y_bm_final, axis= 1)\n",
        "y_bm_final = le_class.inverse_transform(y_bm_final)\n",
        "testDataSub = testDataRaw.copy()\n",
        "testDataSub['Animal'] = y_bm_final\n",
        "testDataSub.to_csv(os.path.join(dataDir, 'submission4 CNN.csv'), index = False)"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkzadlRRBAFS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}